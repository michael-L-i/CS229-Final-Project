{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michael-L-i/CS229-Final-Project/blob/main/Physics_Based_ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyWavelets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_ontbCw48fU",
        "outputId": "23b6f383-864a-40d4-ec3c-cc373fea9684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyWavelets\n",
            "  Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (1.26.4)\n",
            "Downloading pywavelets-1.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/4.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m196.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyWavelets\n",
            "Successfully installed PyWavelets-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6guorDebdnh",
        "outputId": "6d1a2f58-f706-400d-e6b2-4efbdad483f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from PIL import Image\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2\n",
        "import pywt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# SET RANDOMNESS\n",
        "# ========================\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# For CUDA convolution determinism\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# ========================\n",
        "# PARAMETERS AND LOGGING\n",
        "# ========================\n",
        "\n",
        "# Set up logging to both file and console\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "handler.flush = sys.stdout.flush\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('training_log.log'),\n",
        "        handler\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Base configuration dictionary for easy tuning\n",
        "base_config = {\n",
        "    \"resnet_version\": 50,  # Options: 18, 34, 50, 101, 152\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 0.00005,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"num_epochs\": 5,\n",
        "    \"momentum\": 0.9,         # Not used with Adam (but useful for SGD)\n",
        "    \"use_scheduler\": True,\n",
        "    \"train_split\": 0.8       # This is no longer used for splitting\n",
        "}\n",
        "\n",
        "# define num_workers for speedup\n",
        "n_workers = 15\n"
      ],
      "metadata": {
        "id": "8Vz_LZPflOZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WaveletTransform(object):\n",
        "    def __init__(self, wavelet='haar'):\n",
        "        self.wavelet = wavelet\n",
        "\n",
        "    def __call__(self, pil_img):\n",
        "        # Convert PIL Image to NumPy array\n",
        "        img = np.array(pil_img)\n",
        "\n",
        "        # If there's an alpha channel, drop it\n",
        "        if img.shape[-1] == 4:\n",
        "            img = img[..., :3]\n",
        "\n",
        "        # Convert from RGB (PIL) to BGR (OpenCV convention)\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # Convert to grayscale\n",
        "        img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Perform single-level 2D Discrete Wavelet Transform\n",
        "        cA, (cH, cV, cD) = pywt.dwt2(img_gray, self.wavelet)\n",
        "\n",
        "        # Function to normalize wavelet coefficients to [0, 255]\n",
        "        def normalize(channel):\n",
        "            channel = channel.astype(np.float32)\n",
        "            channel_min, channel_max = channel.min(), channel.max()\n",
        "            # Avoid divide-by-zero\n",
        "            channel = (channel - channel_min) / (channel_max - channel_min + 1e-6)\n",
        "            return (channel * 255).astype(np.uint8)\n",
        "\n",
        "        # Normalize detail coefficients\n",
        "        cH = normalize(cH)\n",
        "        cV = normalize(cV)\n",
        "        cD = normalize(cD)\n",
        "\n",
        "        # Stack detail coefficients as 3 channels: (H, V, D) -> (R, G, B)\n",
        "        wavelet_3ch = np.dstack([cH, cV, cD])\n",
        "\n",
        "        # Convert NumPy array back to a PIL Image in RGB mode\n",
        "        wavelet_3ch_pil = Image.fromarray(wavelet_3ch, mode='RGB')\n",
        "\n",
        "        return wavelet_3ch_pil"
      ],
      "metadata": {
        "id": "S9JElICp5DUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# DATASET DEFINITIONS\n",
        "# ========================\n",
        "\n",
        "class RecaptureDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, is_test=False):\n",
        "        \"\"\"\n",
        "        For training/validation/testing, expects two subfolders:\n",
        "        'SingleCaptureImages' (label 0) and 'RecapturedImages' (label 1).\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        self.is_test = is_test\n",
        "\n",
        "        self.single_capture_path = os.path.join(root_dir, 'SingleCaptureImages')\n",
        "        self.recapture_path = os.path.join(root_dir, 'RecapturedImages')\n",
        "        self.single_capture_images = []\n",
        "        self.recapture_images = []\n",
        "        # Get images from SingleCaptureImages\n",
        "        for root, _, files in os.walk(self.single_capture_path):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    self.single_capture_images.append(os.path.join(root, file))\n",
        "        # Get images from RecapturedImages\n",
        "        for root, _, files in os.walk(self.recapture_path):\n",
        "            for file in files:\n",
        "                if file.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    self.recapture_images.append(os.path.join(root, file))\n",
        "        self.all_images = self.single_capture_images + self.recapture_images\n",
        "        self.labels = ([0] * len(self.single_capture_images)) + ([1] * len(self.recapture_images))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.all_images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.all_images[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# ========================\n",
        "# TRANSFORMATIONS\n",
        "# ========================\n",
        "\n",
        "# Transformations for training/validation/testing (includes augmentation for training)\n",
        "train_transform = transforms.Compose([\n",
        "    WaveletTransform(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# ========================\n",
        "# DATASET & DATALOADERS\n",
        "# ========================\n",
        "\n",
        "# Specify your dataset root directory\n",
        "root_dir = \"/content/drive/MyDrive/CS229_datasets/processed\"\n",
        "\n",
        "# Create the full dataset\n",
        "full_dataset = RecaptureDataset(root_dir=root_dir, transform=train_transform, is_test=False)\n",
        "dataset_size = len(full_dataset)\n",
        "train_size = int(0.8 * dataset_size)\n",
        "val_size = int(0.1 * dataset_size)\n",
        "test_size = dataset_size - train_size - val_size\n",
        "logging.info(f\"Total images: {dataset_size}, Training: {train_size}, Validation: {val_size}, Testing: {test_size}\")\n",
        "\n",
        "# Use a torch.Generator with a fixed seed for reproducible splits\n",
        "generator = torch.Generator().manual_seed(seed)\n",
        "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size], generator=generator)\n",
        "\n",
        "def get_dataloaders(config, train_dataset, val_dataset, test_dataset):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True,\n",
        "                              num_workers=n_workers, pin_memory=True, persistent_workers=True,\n",
        "                              prefetch_factor=3)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False,\n",
        "                             num_workers=n_workers, pin_memory=True, persistent_workers=True,\n",
        "                             prefetch_factor=3)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False,\n",
        "                             num_workers=n_workers, pin_memory=True, persistent_workers=True,\n",
        "                             prefetch_factor=3)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "# ========================\n",
        "# MODEL DEFINITION\n",
        "# ========================\n",
        "\n",
        "class RecaptureResNet(nn.Module):\n",
        "    def __init__(self, resnet_version=18):\n",
        "        super(RecaptureResNet, self).__init__()\n",
        "        resnet_models = {\n",
        "            18: models.resnet18,\n",
        "            34: models.resnet34,\n",
        "            50: models.resnet50,\n",
        "            101: models.resnet101,\n",
        "            152: models.resnet152,\n",
        "        }\n",
        "        # Load the pretrained ResNet model (default 3-channel input)\n",
        "        self.model = resnet_models[resnet_version](pretrained=True)\n",
        "\n",
        "        # Modify the fully connected layer for binary classification\n",
        "        num_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(num_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Set up device (GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ========================\n",
        "# TRAINING & EVALUATION FUNCTIONS\n",
        "# ========================\n",
        "\n",
        "def train_and_evaluate(config, train_dataset, val_dataset, test_dataset):\n",
        "    train_loader, val_loader, test_loader = get_dataloaders(config, train_dataset, val_dataset, test_dataset)\n",
        "    model = RecaptureResNet(resnet_version=config[\"resnet_version\"]).to(device)\n",
        "\n",
        "    # Weighted binary cross entropy: Increase weight for class 1\n",
        "    pos_weight = torch.tensor([3.0]).to(device)\n",
        "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1) if config[\"use_scheduler\"] else None\n",
        "\n",
        "    train_losses = []\n",
        "    model.train()\n",
        "    for epoch in range(config[\"num_epochs\"]):\n",
        "        running_loss = 0.0\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config['num_epochs']}\", leave=True):\n",
        "            images, labels = images.to(device), labels.float().to(device)\n",
        "            labels = labels.view(-1, 1)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(epoch_loss)\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "        logging.info(f\"Epoch {epoch+1}/{config['num_epochs']} - Loss: {epoch_loss:.4f}\")\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} - Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        # Evaluate on validation set at the end of each epoch\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in tqdm(val_loader, desc=\"Evaluating on Validation\", leave=True):\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                predictions = (torch.sigmoid(outputs) > 0.5).int()\n",
        "                val_correct += (predictions.view(-1) == labels).sum().item()\n",
        "                val_total += labels.size(0)\n",
        "        val_accuracy = 100 * val_correct / val_total\n",
        "        logging.info(f\"Epoch {epoch+1}/{config['num_epochs']} - Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']} - Validation Accuracy: {val_accuracy:.2f}%\")\n",
        "        model.train()\n",
        "\n",
        "    # Final evaluation on test set\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader, desc=\"Evaluating on Test\", leave=True):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            predictions = (torch.sigmoid(outputs) > 0.5).int()\n",
        "            test_correct += (predictions.view(-1) == labels).sum().item()\n",
        "            test_total += labels.size(0)\n",
        "    test_accuracy = 100 * test_correct / test_total\n",
        "    logging.info(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "    return train_losses, val_accuracy, test_accuracy, model"
      ],
      "metadata": {
        "id": "EEM2BPEMlKWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# SAVE & LOAD MODEL\n",
        "# ========================\n",
        "\n",
        "tuned_config = {\n",
        "    \"resnet_version\": 50,  # Options: 18, 34, 50, 101, 152\n",
        "    \"batch_size\": 128,\n",
        "    \"learning_rate\": 5e-5,\n",
        "    \"weight_decay\": 1e-4,\n",
        "    \"num_epochs\": 15,\n",
        "    \"use_scheduler\": True,\n",
        "    \"train_split\": 0.8       # Not used anymore for splitting\n",
        "}\n",
        "\n",
        "# Train the model with the tuned configuration using the 8:1:1 split\n",
        "_, _, _, model = train_and_evaluate(tuned_config, train_dataset, val_dataset, test_dataset)\n",
        "\n",
        "# Save the model state (only weights)\n",
        "model_path = f'recapture_resnet{tuned_config[\"resnet_version\"]}.pth'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "logging.info(f\"Model saved as {model_path}\")\n",
        "print(f\"Model saved as {model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWt8u9_YlLvo",
        "outputId": "8535ab0b-fbd1-4e95-912d-47bc42c24886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 15 worker processes in total. Our suggested max number of worker in current system is 12, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Epoch 1/15: 100%|██████████| 15/15 [02:32<00:00, 10.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 - Loss: 0.6480\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [01:47<00:00, 53.92s/it] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15 - Validation Accuracy: 40.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/15: 100%|██████████| 15/15 [01:14<00:00,  4.94s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/15 - Loss: 0.0692\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [00:33<00:00, 16.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/15 - Validation Accuracy: 86.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/15: 100%|██████████| 15/15 [01:12<00:00,  4.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/15 - Loss: 0.0219\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [00:34<00:00, 17.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/15 - Validation Accuracy: 97.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/15: 100%|██████████| 15/15 [01:12<00:00,  4.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/15 - Loss: 0.0095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [00:33<00:00, 16.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/15 - Validation Accuracy: 99.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/15: 100%|██████████| 15/15 [01:13<00:00,  4.90s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/15 - Loss: 0.0047\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [00:34<00:00, 17.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/15 - Validation Accuracy: 97.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/15: 100%|██████████| 15/15 [01:15<00:00,  5.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/15 - Loss: 0.0040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [00:34<00:00, 17.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/15 - Validation Accuracy: 97.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/15: 100%|██████████| 15/15 [01:13<00:00,  4.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/15 - Loss: 0.0029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [00:33<00:00, 16.77s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/15 - Validation Accuracy: 97.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/15: 100%|██████████| 15/15 [01:12<00:00,  4.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/15 - Loss: 0.0033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [00:33<00:00, 16.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/15 - Validation Accuracy: 97.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/15: 100%|██████████| 15/15 [01:12<00:00,  4.84s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/15 - Loss: 0.0030\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [00:33<00:00, 16.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/15 - Validation Accuracy: 97.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/15: 100%|██████████| 15/15 [01:13<00:00,  4.92s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/15 - Loss: 0.0027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [00:33<00:00, 16.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/15 - Validation Accuracy: 98.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/15: 100%|██████████| 15/15 [01:11<00:00,  4.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/15 - Loss: 0.0032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [00:33<00:00, 16.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/15 - Validation Accuracy: 97.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/15: 100%|██████████| 15/15 [01:15<00:00,  5.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/15 - Loss: 0.0024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [00:33<00:00, 16.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/15 - Validation Accuracy: 98.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/15: 100%|██████████| 15/15 [01:13<00:00,  4.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/15 - Loss: 0.0029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [00:33<00:00, 16.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/15 - Validation Accuracy: 97.86%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/15: 100%|██████████| 15/15 [01:13<00:00,  4.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/15 - Loss: 0.0026\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [00:33<00:00, 16.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/15 - Validation Accuracy: 98.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/15: 100%|██████████| 15/15 [01:13<00:00,  4.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/15 - Loss: 0.0025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Validation: 100%|██████████| 2/2 [00:33<00:00, 16.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/15 - Validation Accuracy: 98.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating on Test: 100%|██████████| 2/2 [01:43<00:00, 51.85s/it] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 97.86%\n",
            "Model saved as recapture_resnet50.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kPnR3L-R5cU2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}